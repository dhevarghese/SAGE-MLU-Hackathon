{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Color Attribution: Vision-Language Models vs. Language Models\n",
    "\n",
    "This notebook demonstrates how color attribution differs between:\n",
    "1. Vision-Language Models (VLMs) that can directly process images\n",
    "2. Language Models (LMs) that rely on textual descriptions of images\n",
    "\n",
    "We'll use a simple banana image to compare how each model identifies the color of the fruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vision-Language Model Approach (PaliGemma2)\n",
    "\n",
    "PaliGemma2 can directly analyze the image to determine the color of the fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://huggingface.co/google/paligemma2-3b-mix-224\n",
    "\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma2-3b-mix-224\"\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "\n",
    "prompt = \"answer en which color is the fruit?\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n",
    "\n",
    "# -> \"yellow\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Model Approach (Gemma 2)\n",
    "\n",
    "Gemma 2 cannot see the image directly. Instead, it relies on a textual description of the image to determine the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "# !pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "image_desctiption = \"The image shows a bunch of bananas lying on a weathered wooden surface. The bananas are clustered together, still attached at their stems. The wood grain of the surface is visible and adds a rustic feel to the image. \"\n",
    "input_prompt = image_desctiption + \"What is the color of the fruit?\"\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# -> The color of the fruit is **yellow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Both models correctly identify the banana as yellow, but through different mechanisms:\n",
    "- The VLM (PaliGemma2) directly processes the visual information from the image\n",
    "- The LM (Gemma 2) relies on the textual description that explicitly mentions bananas (which it knows are yellow)\n",
    "\n",
    "This demonstrates the difference in how these model types handle visual information and attribute properties like color."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
